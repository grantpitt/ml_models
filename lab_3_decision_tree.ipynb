{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro038g6MmlkH"
   },
   "source": [
    "# Decision Tree Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FudjR6shvxdY",
    "outputId": "d3f09c15-5b07-4022-be95-f3c044d856d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arff\n",
      "  Downloading https://files.pythonhosted.org/packages/50/de/62d4446c5a6e459052c2f2d9490c370ddb6abc0766547b4cef585913598d/arff-0.9.tar.gz\n",
      "Building wheels for collected packages: arff\n",
      "  Building wheel for arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for arff: filename=arff-0.9-cp37-none-any.whl size=4970 sha256=742b239cd187e3112190c9fee00e58255a2f0b984bb926e9c399902818596d97\n",
      "  Stored in directory: /root/.cache/pip/wheels/04/d0/70/2c73afedd3ac25c6085b528742c69b9587cbdfa67e5194583b\n",
      "Successfully built arff\n",
      "Installing collected packages: arff\n",
      "Successfully installed arff-0.9\n"
     ]
    }
   ],
   "source": [
    "pip install arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "XmcYEVNmmlkI"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import uuid\n",
    "import arff as arf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqz38MR_mlkJ"
   },
   "source": [
    "## 1. (40%) Correctly implement the ID3 decision tree algorithm, including the ability to handle unknown attributes (You do not need to handle real valued attributes).  \n",
    "### Code Requirements/Notes:\n",
    "- Use standard information gain as your basic attribute evaluation metric.  (Note that normal ID3 would usually augment information gain with gain ratio or some other mechanism to penalize statistically insignificant attribute splits. Otherwise, even with approaches like pruning below, the SSE type of overfit could still hurt us.) \n",
    "- You are welcome to create other classes and/or functions in addition to the ones provided below. (e.g. If you build out a tree structure, you might create a node class).\n",
    "- It is a good idea to use a simple data set (like the lenses data or the pizza homework), which you can check by hand, to test your algorithm to make sure that it is working correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gFlzJxRLr24L"
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, data, labels, feat=None, val=None):\n",
    "        if len(data) != len(labels):\n",
    "            raise ValueError(\"Error: number of instances does not match number of labels\")\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.feat = feat\n",
    "        self.val = val\n",
    "        self.id = uuid.uuid4()\n",
    "        self.children_ids = []\n",
    "    \n",
    "    def avalable_feats(self, used_feats):\n",
    "        all_feats = [self.feat]\n",
    "        all_feats.extend(used_feats)\n",
    "        feats = np.arange(len(self.data[0]))\n",
    "        return feats[~np.isin(feats, all_feats)]\n",
    "\n",
    "    def info(self):\n",
    "        summation = 0\n",
    "        n = len(self.labels)\n",
    "        for val in np.unique(self.labels):\n",
    "            prob = len(self.labels[self.labels == val]) / n\n",
    "            if prob > 1e-5:\n",
    "                summation += prob * np.log2(prob)\n",
    "        return -summation\n",
    "    \n",
    "    def make_child(self, feat, val):\n",
    "        sieve = self.data[:,feat] == val\n",
    "        return Node(self.data[sieve], self.labels[sieve], feat, val)\n",
    "    \n",
    "    def majority(self):\n",
    "        return np.bincount(self.labels).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1prGeSwzsZiu"
   },
   "outputs": [],
   "source": [
    "class Tree():\n",
    "    def __init__(self, root):\n",
    "        self.nodes = {}\n",
    "        self.root = root\n",
    "        self.push(root)\n",
    "\n",
    "    def push(self, node):\n",
    "        self.nodes[node.id] = node\n",
    "    \n",
    "    def view(self):\n",
    "        self.view_helper(self.root, -1)\n",
    "\n",
    "    def view_helper(self, node, layer):\n",
    "        if node.feat is not None:\n",
    "            print((\" \" * 4 * layer) + \"feature_\" + str(node.feat), \"=\", node.val)\n",
    "        if len(node.children_ids) == 0:\n",
    "            print((\" \" * 4 * (layer + 1)) + \"prediction:\", node.majority())\n",
    "        else:\n",
    "            for child_id in node.children_ids:\n",
    "                child = self.nodes[child_id]\n",
    "                self.view_helper(child, layer + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Di9qmOWZmlkK"
   },
   "outputs": [],
   "source": [
    "class DTClassifier(BaseEstimator,ClassifierMixin):\n",
    "\n",
    "    def __init__(self, counts=None):\n",
    "        \"\"\" Initialize class with chosen hyperparameters.\n",
    "        Args:\n",
    "        Optional Args (Args we think will make your life easier):\n",
    "            counts: A list of Ints that tell you how many types of each feature there are\n",
    "        Example:\n",
    "            DT  = DTClassifier()\n",
    "            or\n",
    "            DT = DTClassifier(count = [2,3,2,2])\n",
    "            Dataset = \n",
    "            [[0,1,0,0],\n",
    "            [1,2,1,1],\n",
    "            [0,1,1,0],\n",
    "            [1,2,0,1],\n",
    "            [0,0,1,1]]\n",
    "\n",
    "        \"\"\"\n",
    "        self.counts = counts\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data; Make the Decision tree\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "            y (array-like): A 1D numpy array with the training targets\n",
    "\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "\n",
    "        \"\"\"\n",
    "        root = Node(X, y)\n",
    "        self.tree = Tree(root)\n",
    "        self.split_gains = []\n",
    "        self.partition_node(root)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def partition_node(self, node, used_feats=[]):\n",
    "        avalable_feats = node.avalable_feats(used_feats)\n",
    "        # Make sure there are still feature to split on\n",
    "        if len(avalable_feats) == 0:\n",
    "            return\n",
    "\n",
    "        # Find the best feature to split on using gain as the metric\n",
    "        n = len(node.data)\n",
    "        info = node.info()\n",
    "        scores, children = {}, {}\n",
    "        for i in avalable_feats:\n",
    "            feature_info = 0\n",
    "            feature_children = []\n",
    "            for val in np.unique(node.data[:,i]):\n",
    "                child = node.make_child(i, val)\n",
    "                feature_children.append(child)\n",
    "                m = len(child.data)\n",
    "                feature_info += (m/n) * child.info()\n",
    "            # Keep track of child nodes and gain\n",
    "            children[i] = feature_children\n",
    "            scores[i] = info - feature_info\n",
    "        \n",
    "        # Find feature that maximizes gain\n",
    "        best_feature = max(scores, key=scores.get)\n",
    "        gain = scores[best_feature]\n",
    "        if gain < 1e-5:\n",
    "            return\n",
    "        self.split_gains.append(gain)\n",
    "\n",
    "        # Add nodes to tree and call partition on them\n",
    "        for leaf in children[best_feature]:\n",
    "            self.tree.push(leaf)\n",
    "            node.children_ids.append(leaf.id)\n",
    "            self.partition_node(leaf, [leaf.feat])\n",
    "\n",
    "    def view_tree(self):\n",
    "        self.tree.view()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict all classes for a dataset X\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "\n",
    "        Returns:\n",
    "            array, shape (n_samples,)\n",
    "                Predicted target values per element in X.\n",
    "        \"\"\"\n",
    "        return np.array([self.single_predict(pattern) for pattern in X])\n",
    "\n",
    "    def single_predict(self, x):\n",
    "        current_node = self.tree.root\n",
    "        while len(current_node.children_ids) > 0:\n",
    "            found = False\n",
    "            for id in current_node.children_ids:\n",
    "                child = self.tree.nodes[id]\n",
    "                if x[child.feat] == child.val:\n",
    "                    current_node = child\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                break\n",
    "        return current_node.majority()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Return accuracy(Classification Acc) of model on a given dataset. Must implement own score function.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 1D numpy array of the targets \n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        correct = sum([1 for i, z in enumerate(predictions) if z == y[i]])\n",
    "        return correct / len(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E94Sjw2WmlkM"
   },
   "source": [
    "## 1.1 Debug\n",
    "\n",
    "Debug your model by training on the lenses dataset: [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses.arff)\n",
    "\n",
    "Test your model on the lenses test set: [Debug Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses_test.arff)\n",
    "\n",
    "Parameters:\n",
    "(optional) counts = [3,2,2,2] (You should compute this when you read in the data, before fitting)\n",
    "\n",
    "---\n",
    "\n",
    "Expected Results: Accuracy = [0.33]\n",
    "\n",
    "Predictions should match this file: [Lenses Predictions](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv)\n",
    "\n",
    "*NOTE: The [Lenses Prediction](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv) uses the following encoding: soft=2, hard=0, none=1. If your encoding is different, then your output will be different, but not necessarily incorrect.*\n",
    "\n",
    "Split Information Gains (These do not need to be in this exact order):\n",
    "\n",
    "[0.5487949406953987, 0.7704260414863775, 0.3166890883150208, 1.0, 0.4591479170272447, 0.9182958340544894]\n",
    "\n",
    "<!-- You should be able to get about 68% (61%-82%) predictive accuracy on the lenses data -->\n",
    "\n",
    "Here's what your decision tree splits should look like, and the corresponding child node predictions:\n",
    "\n",
    "Decision Tree:\n",
    "<pre>\n",
    "feature_3 = 0:\n",
    "\tfeature_2 = 0:\n",
    "\t\tfeature_0 = 0:\n",
    "\t\t\tprediction: 2\n",
    "\t\tfeature_0 = 1:\n",
    "\t\t\tfeature_1 = 0:\n",
    "\t\t\t\tprediction: 2\n",
    "\t\t\tfeature_1 = 1:\n",
    "\t\t\t\tprediction: 1\n",
    "\t\tfeature_0 = 2:\n",
    "\t\t\tprediction: 2\n",
    "\tfeature_2 = 1:\n",
    "\t\tfeature_1 = 0:\n",
    "\t\t\tfeature_0 = 0:\n",
    "\t\t\t\tprediction: 1\n",
    "\t\t\tfeature_0 = 1:\n",
    "\t\t\t\tprediction: 1\n",
    "\t\t\tfeature_0 = 2:\n",
    "\t\t\t\tprediction: 0\n",
    "\t\tfeature_1 = 1:\n",
    "\t\t\tprediction: 0\n",
    "feature_3 = 1:\n",
    "\tprediction: 1\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUaCIvgnmlkN",
    "outputId": "823ea844-727c-4818-cc0e-0049899760e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_3 = 0\n",
      "    feature_2 = 0\n",
      "        feature_0 = 0\n",
      "            prediction: 2\n",
      "        feature_0 = 1\n",
      "            feature_1 = 0\n",
      "                prediction: 2\n",
      "            feature_1 = 1\n",
      "                prediction: 1\n",
      "        feature_0 = 2\n",
      "            prediction: 2\n",
      "    feature_2 = 1\n",
      "        feature_1 = 0\n",
      "            feature_0 = 0\n",
      "                prediction: 1\n",
      "            feature_0 = 1\n",
      "                prediction: 1\n",
      "            feature_0 = 2\n",
      "                prediction: 0\n",
      "        feature_1 = 1\n",
      "            prediction: 0\n",
      "feature_3 = 1\n",
      "    prediction: 1\n",
      "Accuracy: 0.33\n",
      "[0.5487949406953982, 0.7704260414863778, 0.3166890883150208, 1.0, 0.4591479170272448, 0.9182958340544896]\n"
     ]
    }
   ],
   "source": [
    "# Load debug training data \n",
    "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses.arff --output lenses.arff &> /dev/null\n",
    "lenses_data = arff.loadarff(\"lenses.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "lenses_df = pd.DataFrame(lenses_data[0])\n",
    "lenses_df = lenses_df.apply(LabelEncoder().fit_transform)\n",
    "lenses_np = np.array(lenses_df)\n",
    "# Split off labels\n",
    "lenses_x, lenses_y = lenses_np[:,:-1], lenses_np[:,-1]\n",
    "\n",
    "# Train Decision Tree\n",
    "lenses_DT = DTClassifier()\n",
    "lenses_DT.fit(lenses_x, lenses_y)\n",
    "lenses_DT.view_tree()\n",
    "\n",
    "# Load debug test data\n",
    "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses_test.arff --output lenses_test.arff &> /dev/null\n",
    "lenses_test_data = arff.loadarff(\"lenses_test.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "lenses_test_df = pd.DataFrame(lenses_test_data[0])\n",
    "lenses_test_df = lenses_test_df.apply(LabelEncoder().fit_transform)\n",
    "lenses_test_np = np.array(lenses_test_df)\n",
    "# Split off labels\n",
    "lenses_test_x, lenses_test_y = lenses_test_np[:,:-1], lenses_test_np[:,-1]\n",
    "\n",
    "# Predict and compute model accuracy\n",
    "accuracy = lenses_DT.score(lenses_test_x, lenses_test_y)\n",
    "print(\"Accuracy:\", round(accuracy, 2))\n",
    "\n",
    "# Print the information gain of every split you make.\n",
    "print(lenses_DT.split_gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSqht2OJmlkP",
    "outputId": "cee24867-0c88-404a-ac0a-9c3667d8b0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_0 = 0\n",
      "    feature_2 = 0\n",
      "        prediction: 0\n",
      "    feature_2 = 1\n",
      "        prediction: 1\n",
      "feature_0 = 1\n",
      "    feature_1 = 0\n",
      "        feature_2 = 0\n",
      "            prediction: 1\n",
      "        feature_2 = 1\n",
      "            prediction: 2\n",
      "    feature_1 = 1\n",
      "        prediction: 2\n",
      "    feature_1 = 2\n",
      "        prediction: 2\n"
     ]
    }
   ],
   "source": [
    "# Optional/Additional Debugging Dataset - Pizza Homework\n",
    "pizza_dataset = np.array([[1,2,0],[0,0,0],[0,1,1],[1,1,1],[1,0,0],[1,0,1],[0,2,1],[1,0,0],[0,2,0]])\n",
    "pizza_labels = np.array([2,0,1,2,1,2,1,1,0])\n",
    "pizza_DT = DTClassifier()\n",
    "pizza_DT.fit(pizza_dataset, pizza_labels)\n",
    "pizza_DT.view_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af5oBYA_mlkQ"
   },
   "source": [
    "## 1.2 Evaluation\n",
    "\n",
    "We will evaluate your model based on its performance on the zoo dataset. \n",
    "\n",
    "Train your model using this dataset: [Evaluation Train Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo.arff)\n",
    "\n",
    "Test your model on this dataset: [Evaluation Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo_test.arff)\n",
    "\n",
    "Parameters:\n",
    "(optional) counts = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2] (You should compute this when you read in the data, before fitting)\n",
    "\n",
    "---\n",
    "Print out your accuracy on the evaluation test dataset.\n",
    "\n",
    "Print out the information gain of every split you make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhRXMKPHmlkR",
    "outputId": "cae6283b-59d2-4c75-e83e-954e4f0fbe8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_12 = 0\n",
      "    feature_11 = 0\n",
      "        feature_7 = 0\n",
      "            prediction: 5\n",
      "        feature_7 = 1\n",
      "            prediction: 1\n",
      "    feature_11 = 1\n",
      "        feature_2 = 0\n",
      "            prediction: 6\n",
      "        feature_2 = 1\n",
      "            prediction: 2\n",
      "feature_12 = 1\n",
      "    feature_0 = 0\n",
      "        prediction: 0\n",
      "    feature_0 = 1\n",
      "        prediction: 6\n",
      "feature_12 = 2\n",
      "    feature_0 = 0\n",
      "        feature_5 = 0\n",
      "            prediction: 1\n",
      "        feature_5 = 1\n",
      "            feature_7 = 0\n",
      "                prediction: 5\n",
      "            feature_7 = 1\n",
      "                prediction: 3\n",
      "    feature_0 = 1\n",
      "        prediction: 6\n",
      "feature_12 = 3\n",
      "    prediction: 5\n",
      "feature_12 = 4\n",
      "    feature_5 = 0\n",
      "        prediction: 4\n",
      "    feature_5 = 1\n",
      "        prediction: 5\n",
      "feature_12 = 5\n",
      "    prediction: 5\n",
      "Accuracy: 0.15\n",
      "[1.3630469031539394, 0.8865408928220899, 0.9852281360342515, 0.6962122601251459, 0.8256265261578954, 0.6892019851173656, 0.8631205685666308, 0.7219280948873623, 0.7219280948873623]\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation training data\n",
    "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo.arff --output zoo.arff &> /dev/null\n",
    "zoo_data = arff.loadarff(\"zoo.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "zoo_df = pd.DataFrame(zoo_data[0])\n",
    "zoo_df = zoo_df.apply(LabelEncoder().fit_transform)\n",
    "zoo_np = np.array(zoo_df)\n",
    "# Split off labels\n",
    "zoo_x, zoo_y = zoo_np[:,:-1], zoo_np[:,-1]\n",
    "\n",
    "# Train Decision Tree\n",
    "zoo_DT = DTClassifier()\n",
    "zoo_DT.fit(zoo_x, zoo_y)\n",
    "zoo_DT.view_tree()\n",
    "\n",
    "# Load evaluation test data\n",
    "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo_test.arff --output zoo_test.arff &> /dev/null\n",
    "zoo_test_data = arff.loadarff(\"zoo_test.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "zoo_test_df = pd.DataFrame(zoo_test_data[0])\n",
    "zoo_test_df = zoo_test_df.apply(LabelEncoder().fit_transform)\n",
    "zoo_test_np = np.array(zoo_test_df)\n",
    "# Split off labels\n",
    "zoo_test_x, zoo_test_y = zoo_test_np[:,:-1], zoo_test_np[:,-1]\n",
    "\n",
    "# Predict and compute model accuracy\n",
    "accuracy = zoo_DT.score(zoo_test_x, zoo_test_y)\n",
    "print(\"Accuracy:\", round(accuracy, 2))\n",
    "\n",
    "# Print out the information gain for every split you make\n",
    "print(zoo_DT.split_gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDIfGlpQmlkS"
   },
   "source": [
    "## 2. (20%) You will use your ID3 algorithm to induce decision trees for the cars dataset and the voting dataset.  Do not use a stopping criteria, but induce the tree as far as it can go (until classes are pure or there are no more data or attributes to split on).  \n",
    "- Implement and use 10-fold Cross Validation (CV) on each data set to predict how well the models will do on novel data.  \n",
    "- For each dataset, report the training and test classification accuracy for each fold and the average test accuracy. \n",
    "- As a rough sanity check, typical decision tree accuracies for these data sets are: Cars: .90-.95, Vote: .92-.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "462aQmejmlkS"
   },
   "source": [
    "## 2.1 Implement 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "4zt6M4ySmlkS"
   },
   "outputs": [],
   "source": [
    "# Write a function that implements 10-fold cross validation\n",
    "def cross_val(data):\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    k = 10\n",
    "    n = len(data)\n",
    "    part = n // k\n",
    "    test_accs = []\n",
    "    DT = None\n",
    "    for i in range(k):\n",
    "        mask = np.ones(len(data), dtype=bool)\n",
    "        mask[(i * part):((i + 1) * part)] = False\n",
    "\n",
    "        test, val = data[mask], data[~mask]\n",
    "\n",
    "        DT = DTClassifier()\n",
    "        DT.fit(test[:,:-1], test[:,-1])\n",
    "        train_acc = DT.score(test[:,:-1], test[:,-1])\n",
    "        test_acc = DT.score(val[:,:-1], val[:,-1])\n",
    "        test_accs.append(test_acc)\n",
    "        print(\"Partition #\" + str(i+1) + \": train acc:\", train_acc, \"| test acc:\", test_acc)\n",
    "    print(\"Average test acc\", sum(test_accs) / k)\n",
    "    return DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbmFlW7smlkT"
   },
   "source": [
    "##  2.2 Cars Dataset\n",
    "- Use this [Cars Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/cars.arff)\n",
    "- Make a table for your K-Fold cross validation accuracies\n",
    "\n",
    "*If you are having trouble using scipy's loadarff function (scipy.io.arff.loadarff), try:*\n",
    "\n",
    "*pip install arff &nbsp;&nbsp;&nbsp;&nbsp;          # Install arff library*\n",
    "\n",
    "*import arff as arf*                   \n",
    "\n",
    "*cars = list(arf.load('cars.arff'))   &nbsp;&nbsp;&nbsp;&nbsp;# Load your downloaded dataset (!curl, etc.)*\n",
    "\n",
    "*df = pd.DataFrame(cars)*  \n",
    "\n",
    "*There may be additional cleaning needed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzQ-tLqFmlkT",
    "outputId": "7b047c6c-0b62-4e32-8c5b-fdd7187bc1e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1: train acc: 1.0 | test acc: 0.9534883720930233\n",
      "Partition #2: train acc: 1.0 | test acc: 0.9476744186046512\n",
      "Partition #3: train acc: 1.0 | test acc: 0.9418604651162791\n",
      "Partition #4: train acc: 1.0 | test acc: 0.9244186046511628\n",
      "Partition #5: train acc: 1.0 | test acc: 0.9476744186046512\n",
      "Partition #6: train acc: 1.0 | test acc: 0.9476744186046512\n",
      "Partition #7: train acc: 1.0 | test acc: 0.9593023255813954\n",
      "Partition #8: train acc: 1.0 | test acc: 0.9418604651162791\n",
      "Partition #9: train acc: 1.0 | test acc: 0.9418604651162791\n",
      "Partition #10: train acc: 1.0 | test acc: 0.9534883720930233\n",
      "Average test acc 0.9459302325581396\n",
      "feature_5 = 0\n",
      "    feature_3 = 0\n",
      "        prediction: 2\n",
      "    feature_3 = 1\n",
      "        feature_0 = 0\n",
      "            feature_1 = 0\n",
      "                prediction: 0\n",
      "            feature_1 = 1\n",
      "                prediction: 0\n",
      "            feature_1 = 2\n",
      "                prediction: 0\n",
      "            feature_1 = 3\n",
      "                prediction: 2\n",
      "        feature_0 = 1\n",
      "            feature_1 = 0\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    prediction: 0\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    prediction: 1\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    prediction: 1\n",
      "            feature_1 = 3\n",
      "                prediction: 0\n",
      "        feature_0 = 2\n",
      "            feature_1 = 0\n",
      "                prediction: 0\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    prediction: 1\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    prediction: 0\n",
      "            feature_1 = 3\n",
      "                prediction: 0\n",
      "        feature_0 = 3\n",
      "            feature_1 = 0\n",
      "                prediction: 2\n",
      "            feature_1 = 1\n",
      "                prediction: 0\n",
      "            feature_1 = 2\n",
      "                prediction: 0\n",
      "            feature_1 = 3\n",
      "                prediction: 2\n",
      "    feature_3 = 2\n",
      "        feature_0 = 0\n",
      "            feature_1 = 0\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 1\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "            feature_1 = 1\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "            feature_1 = 2\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 1\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "            feature_1 = 3\n",
      "                prediction: 2\n",
      "        feature_0 = 1\n",
      "            feature_1 = 0\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 1\n",
      "                        prediction: 3\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 3\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                    feature_2 = 3\n",
      "                        prediction: 1\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 3\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "            feature_1 = 3\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    prediction: 0\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "        feature_0 = 2\n",
      "            feature_1 = 0\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 3\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                    feature_2 = 3\n",
      "                        prediction: 1\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 3\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 3\n",
      "                    feature_2 = 3\n",
      "                        prediction: 3\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "            feature_1 = 3\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 1\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "        feature_0 = 3\n",
      "            feature_1 = 0\n",
      "                prediction: 2\n",
      "            feature_1 = 1\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 1\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    prediction: 0\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "            feature_1 = 3\n",
      "                prediction: 2\n",
      "feature_5 = 1\n",
      "    prediction: 2\n",
      "feature_5 = 2\n",
      "    feature_3 = 0\n",
      "        prediction: 2\n",
      "    feature_3 = 1\n",
      "        feature_0 = 0\n",
      "            feature_4 = 0\n",
      "                feature_1 = 0\n",
      "                    prediction: 0\n",
      "                feature_1 = 1\n",
      "                    prediction: 0\n",
      "                feature_1 = 2\n",
      "                    prediction: 0\n",
      "                feature_1 = 3\n",
      "                    prediction: 2\n",
      "            feature_4 = 1\n",
      "                feature_2 = 0\n",
      "                    prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 2\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    feature_1 = 0\n",
      "                        prediction: 0\n",
      "                    feature_1 = 1\n",
      "                        prediction: 0\n",
      "                    feature_1 = 2\n",
      "                        prediction: 0\n",
      "                    feature_1 = 3\n",
      "                        prediction: 2\n",
      "            feature_4 = 2\n",
      "                prediction: 2\n",
      "        feature_0 = 1\n",
      "            feature_1 = 0\n",
      "                prediction: 0\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 1\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                    feature_2 = 3\n",
      "                        prediction: 1\n",
      "                feature_4 = 2\n",
      "                    prediction: 0\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 1\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                    feature_2 = 3\n",
      "                        prediction: 1\n",
      "                feature_4 = 2\n",
      "                    prediction: 0\n",
      "            feature_1 = 3\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "        feature_0 = 2\n",
      "            feature_1 = 0\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 1\n",
      "                        prediction: 2\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 1\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                    feature_2 = 3\n",
      "                        prediction: 1\n",
      "                feature_4 = 2\n",
      "                    prediction: 0\n",
      "            feature_1 = 2\n",
      "                prediction: 0\n",
      "            feature_1 = 3\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 2\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "        feature_0 = 3\n",
      "            feature_1 = 0\n",
      "                prediction: 2\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 2\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 2\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "            feature_1 = 3\n",
      "                prediction: 2\n",
      "    feature_3 = 2\n",
      "        feature_0 = 0\n",
      "            feature_4 = 0\n",
      "                feature_1 = 0\n",
      "                    prediction: 0\n",
      "                feature_1 = 1\n",
      "                    prediction: 0\n",
      "                feature_1 = 2\n",
      "                    prediction: 0\n",
      "                feature_1 = 3\n",
      "                    prediction: 2\n",
      "            feature_4 = 1\n",
      "                feature_1 = 0\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_1 = 1\n",
      "                    prediction: 0\n",
      "                feature_1 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_1 = 3\n",
      "                    prediction: 2\n",
      "            feature_4 = 2\n",
      "                prediction: 2\n",
      "        feature_0 = 1\n",
      "            feature_1 = 0\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 1\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 1\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                    feature_2 = 3\n",
      "                        prediction: 1\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "            feature_1 = 3\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "        feature_0 = 2\n",
      "            feature_1 = 0\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 1\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 0\n",
      "                    feature_2 = 1\n",
      "                        prediction: 1\n",
      "                    feature_2 = 2\n",
      "                        prediction: 1\n",
      "                    feature_2 = 3\n",
      "                        prediction: 1\n",
      "                feature_4 = 2\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "            feature_1 = 2\n",
      "                feature_2 = 0\n",
      "                    feature_4 = 0\n",
      "                        prediction: 0\n",
      "                    feature_4 = 1\n",
      "                        prediction: 0\n",
      "                    feature_4 = 2\n",
      "                        prediction: 2\n",
      "                feature_2 = 1\n",
      "                    prediction: 0\n",
      "                feature_2 = 2\n",
      "                    prediction: 0\n",
      "                feature_2 = 3\n",
      "                    prediction: 0\n",
      "            feature_1 = 3\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "        feature_0 = 3\n",
      "            feature_1 = 0\n",
      "                prediction: 2\n",
      "            feature_1 = 1\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "            feature_1 = 2\n",
      "                feature_4 = 0\n",
      "                    prediction: 0\n",
      "                feature_4 = 1\n",
      "                    feature_2 = 0\n",
      "                        prediction: 2\n",
      "                    feature_2 = 1\n",
      "                        prediction: 0\n",
      "                    feature_2 = 2\n",
      "                        prediction: 0\n",
      "                    feature_2 = 3\n",
      "                        prediction: 0\n",
      "                feature_4 = 2\n",
      "                    prediction: 2\n",
      "            feature_1 = 3\n",
      "                prediction: 2\n"
     ]
    }
   ],
   "source": [
    "# Use 10-fold CV on Cars Dataset\n",
    "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/cars.arff --output cars.arff &> /dev/null\n",
    "cars_data = list(arf.load(\"cars.arff\")) # arff.loadarff(\"cars.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "cars_df = pd.DataFrame(cars_data)\n",
    "cars_df = cars_df.apply(LabelEncoder().fit_transform)\n",
    "cars_np = np.array(cars_df)\n",
    "\n",
    "# Report Training and Test Classification Accuracies\n",
    "# Report Average Test Accuracy\n",
    "dt = cross_val(cars_np)\n",
    "dt.view_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm3EknHmlkU"
   },
   "source": [
    "## 2.3 Voting Dataset\n",
    "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)\n",
    "- Note that you will need to support unknown attributes in the voting data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OboZV1damlkU",
    "outputId": "e7f71950-c1ae-4cc6-e4c3-63a01184bfd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1: train acc: 1.0 | test acc: 0.9302325581395349\n",
      "Partition #2: train acc: 1.0 | test acc: 0.9302325581395349\n",
      "Partition #3: train acc: 1.0 | test acc: 0.9534883720930233\n",
      "Partition #4: train acc: 1.0 | test acc: 0.9767441860465116\n",
      "Partition #5: train acc: 1.0 | test acc: 0.9069767441860465\n",
      "Partition #6: train acc: 1.0 | test acc: 0.9302325581395349\n",
      "Partition #7: train acc: 1.0 | test acc: 0.9302325581395349\n",
      "Partition #8: train acc: 1.0 | test acc: 0.9767441860465116\n",
      "Partition #9: train acc: 1.0 | test acc: 0.9534883720930233\n",
      "Partition #10: train acc: 1.0 | test acc: 0.8372093023255814\n",
      "Average test acc 0.9325581395348838\n",
      "feature_3 = 0\n",
      "    feature_8 = 0\n",
      "        prediction: 1\n",
      "    feature_8 = 1\n",
      "        prediction: 0\n",
      "    feature_8 = 2\n",
      "        feature_6 = 0\n",
      "            prediction: 0\n",
      "        feature_6 = 1\n",
      "            prediction: 1\n",
      "        feature_6 = 2\n",
      "            prediction: 0\n",
      "feature_3 = 1\n",
      "    feature_2 = 0\n",
      "        prediction: 0\n",
      "    feature_2 = 1\n",
      "        feature_5 = 0\n",
      "            prediction: 0\n",
      "        feature_5 = 1\n",
      "            feature_14 = 1\n",
      "                prediction: 1\n",
      "            feature_14 = 2\n",
      "                prediction: 0\n",
      "        feature_5 = 2\n",
      "            prediction: 0\n",
      "    feature_2 = 2\n",
      "        prediction: 0\n",
      "feature_3 = 2\n",
      "    feature_10 = 0\n",
      "        prediction: 1\n",
      "    feature_10 = 1\n",
      "        feature_14 = 0\n",
      "            prediction: 1\n",
      "        feature_14 = 1\n",
      "            feature_2 = 1\n",
      "                prediction: 1\n",
      "            feature_2 = 2\n",
      "                feature_15 = 0\n",
      "                    prediction: 0\n",
      "                feature_15 = 2\n",
      "                    prediction: 1\n",
      "        feature_14 = 2\n",
      "            feature_9 = 1\n",
      "                feature_15 = 0\n",
      "                    feature_1 = 1\n",
      "                        prediction: 0\n",
      "                    feature_1 = 2\n",
      "                        prediction: 1\n",
      "                feature_15 = 1\n",
      "                    prediction: 1\n",
      "                feature_15 = 2\n",
      "                    prediction: 0\n",
      "            feature_9 = 2\n",
      "                prediction: 1\n",
      "    feature_10 = 2\n",
      "        feature_2 = 0\n",
      "            prediction: 0\n",
      "        feature_2 = 1\n",
      "            feature_11 = 0\n",
      "                prediction: 1\n",
      "            feature_11 = 1\n",
      "                prediction: 0\n",
      "            feature_11 = 2\n",
      "                feature_15 = 0\n",
      "                    prediction: 1\n",
      "                feature_15 = 1\n",
      "                    feature_0 = 1\n",
      "                        prediction: 0\n",
      "                    feature_0 = 2\n",
      "                        prediction: 1\n",
      "                feature_15 = 2\n",
      "                    prediction: 1\n",
      "        feature_2 = 2\n",
      "            feature_6 = 1\n",
      "                prediction: 0\n",
      "            feature_6 = 2\n",
      "                prediction: 1\n"
     ]
    }
   ],
   "source": [
    "# Used 10-fold CV on Voting Dataset\n",
    "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff --output voting_with_missing.arff &> /dev/null\n",
    "voting_data = arff.loadarff(\"voting_with_missing.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "voting_df = pd.DataFrame(voting_data[0])\n",
    "voting_df = voting_df.apply(LabelEncoder().fit_transform)\n",
    "voting_np = np.array(voting_df)\n",
    "\n",
    "# Report Training and Test Classification Accuracies\n",
    "# Report Average Test Accuracy\n",
    "dt = cross_val(voting_np)\n",
    "dt.view_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeyW9WfrmlkU"
   },
   "source": [
    "## 2.4 Discuss Your Results\n",
    "\n",
    "- Summarize your results from both datasets, and discuss what you observed. \n",
    "- A fully expanded tree will often get 100% accuracy on the training set. Why does this happen and in what cases might it not?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OK8kD9umlkU"
   },
   "source": [
    "Both the cars and voting classification performed very well at 95% and 93% repectively. The cars classifier varied ~3% across the 10-fold cross-val while the voting classifier varied ~13%.\n",
    "\n",
    "If a dataset has alot of different features and values for those features, it's possible that no two inputs are exactly the same so each would end up in a node by itself which would have 100% accuracy. In the case when there is sufficient data with catagorical features, this is unlikely to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIGbkEVwmlkU"
   },
   "source": [
    "## 3. (15%) For each of the two problems above, summarize in English what the decision tree has learned (i.e. look at the induced tree and describe what rules it has discovered to try to solve each task). \n",
    "- If the tree is very large you can just discuss a few of the more shallow attribute combinations and the most important decisions made high in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6Pm_sYZmlkU"
   },
   "source": [
    "## 3.1 Discuss what the decision tree induced on the cars dataset has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQKX389DmlkV"
   },
   "source": [
    "The decision tree on the cars dataset learned that the most important features are saftey, # of seating, and buying price. Saftey and # of seating were often enough to provide an accepability classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeW2ypFsmlkV"
   },
   "source": [
    "## 3.2 Discuss what the decision tree induced on the voting dataset has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwGG9O7emlkV"
   },
   "source": [
    "The decision tree on the voting dataset learned that the most important features are physician-fee-freeze, mx-missile, adoption-of-the-budget-resolution, and synfuels-corporation-cutback. For example, voters who said no to physician-fee-freeze and mx-missile were classified as republican."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NrYBHozmlkW"
   },
   "source": [
    "## 3.3 How did you handle unknown attributes in the voting problem? Why did you choose this approach? (Do not use the approach of just throwing out data with unknown attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6PcentomlkW"
   },
   "source": [
    "I made missing values a seperate value in the features with them. Using scikit's label encoder, this was pretty straight-forward. This probably makes more sense in the case of the voting dataset more than the cars but, depending on how the data was collected, a missing attribute may accually have meaning in the context of the problem. For example, it could mean that a voter was indecisive on an issue. This tells us something about them that could have predictive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu2QbwwHmlkW"
   },
   "source": [
    "## 4.1 (10%) Use SciKit Learn's decision tree on the voting dataset and compare your results. Try different parameters and report what parameters perform the best on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRcX95j3mlkW"
   },
   "source": [
    "### 4.1.1 SK Learn on Voting Dataset\n",
    "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLXwBHxemlkW",
    "outputId": "24ad651a-12c2-4a21-d0f6-707c9c390c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9425287356321839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use SK Learn's Decision Tree to learn the voting dataset\n",
    "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff --output voting_with_missing.arff &> /dev/null\n",
    "voting_data = arff.loadarff(\"voting_with_missing.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "voting_df = pd.DataFrame(voting_data[0])\n",
    "voting_df = voting_df.apply(LabelEncoder().fit_transform)\n",
    "voting_np = np.array(voting_df)\n",
    "# Shuffle data\n",
    "np.random.shuffle(voting_np)\n",
    "# Split off test set\n",
    "part = round(len(voting_np) * .8)\n",
    "voting_train, voting_test = voting_np[:part], voting_np[part:]\n",
    "# Split off labels\n",
    "voting_train_x, voting_train_y = voting_train[:,:-1], voting_train[:,-1]\n",
    "voting_test_x, voting_test_y = voting_test[:,:-1], voting_test[:,-1]\n",
    "\n",
    "# Explore different parameters\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[3, 5, 10, 20]}\n",
    "voting_dt = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(voting_dt, parameters)\n",
    "clf.fit(voting_train_x, voting_train_y)\n",
    "\n",
    "# Report results\n",
    "print(clf.score(voting_test_x, voting_test_y))\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D64xlfuomlkW"
   },
   "source": [
    "The entropy criterion with max depth of 3 had the best results with the voting dataset. Compared to my classifier, it scored ~1% better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS238z1PmlkX"
   },
   "source": [
    "## 4.2 (10%) Choose a data set of your choice (not already used in this or previous labs) and use the SK decision tree to learn it. Experiment with different hyper-parameters to try to get the best results possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOHxO9qZmlkX",
    "outputId": "73a42400-0f4e-48cf-a3ba-9450752af667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7733333333333333\n"
     ]
    }
   ],
   "source": [
    "# Use SciKit Learn's Decision Tree on a new dataset\n",
    "!curl https://raw.githubusercontent.com/grantpitt/ml-datasets/master/transfusion.data --output transfusion.data &> /dev/null\n",
    "autism_data = pd.read_csv(\"transfusion.data\")  # arff.loadarff(\"Autism_Data.arff\")\n",
    "\n",
    "# Encode categorical to numeric\n",
    "autism_df = pd.DataFrame(autism_data)\n",
    "\n",
    "autism_df = autism_df.apply(LabelEncoder().fit_transform)\n",
    "autism_np = np.array(autism_df)\n",
    "# Shuffle data\n",
    "np.random.shuffle(autism_np)\n",
    "# Split off test set\n",
    "part = round(len(autism_np) * .8)\n",
    "autism_train, autism_test = autism_np[:part], autism_np[part:]\n",
    "# Split off labels\n",
    "autism_train_x, autism_train_y = autism_train[:,:-1], autism_train[:,-1]\n",
    "autism_test_x, autism_test_y = autism_test[:,:-1], autism_test[:,-1]\n",
    "\n",
    "# Explore different parameters\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[3, 5, 10, 20]}\n",
    "autism_dt = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(autism_dt, parameters)\n",
    "clf.fit(autism_train_x, autism_train_y)\n",
    "print(clf.score(autism_test_x, autism_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EPAtbCHmlkX"
   },
   "source": [
    "## 5. (5%) Visualize sklearn's decision tree for your chosen data set (using export_graphviz or another tool) and discuss what you find. If your tree is too deep to reasonably fit on one page, show only the first several levels (e.g. top 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "ut68zVDqmlkX",
    "outputId": "aa932856-8b03-43e9-b100-383fc7e9b24b"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"999pt\" height=\"433pt\"\n",
       " viewBox=\"0.00 0.00 999.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-429 995,-429 995,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"#edaa79\" stroke=\"#000000\" points=\"563.5,-425 438.5,-425 438.5,-342 563.5,-342 563.5,-425\"/>\n",
       "<text text-anchor=\"middle\" x=\"501\" y=\"-409.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Recency &lt;= 6.5</text>\n",
       "<text text-anchor=\"middle\" x=\"501\" y=\"-394.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.802</text>\n",
       "<text text-anchor=\"middle\" x=\"501\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 598</text>\n",
       "<text text-anchor=\"middle\" x=\"501\" y=\"-364.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [452, 146]</text>\n",
       "<text text-anchor=\"middle\" x=\"501\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#f5ceb2\" stroke=\"#000000\" points=\"435,-306 311,-306 311,-223 435,-223 435,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-290.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Monetary &lt;= 3.5</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.957</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-260.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 301</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [187, 114]</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M456.2318,-341.8796C446.2537,-332.6031 435.5881,-322.6874 425.3358,-313.1559\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"427.6663,-310.5436 417.9593,-306.2981 422.9,-315.6704 427.6663,-310.5436\"/>\n",
       "<text text-anchor=\"middle\" x=\"418.8696\" y=\"-327.5815\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#e89051\" stroke=\"#000000\" points=\"685.5,-306 568.5,-306 568.5,-223 685.5,-223 685.5,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-290.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Recency &lt;= 21.5</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.493</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-260.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 297</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [265, 32]</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M545.0687,-341.8796C554.7955,-332.6931 565.1861,-322.8798 575.1879,-313.4336\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"577.8763,-315.7089 582.7432,-306.2981 573.0699,-310.6198 577.8763,-315.7089\"/>\n",
       "<text text-anchor=\"middle\" x=\"582.0292\" y=\"-327.5879\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#eda977\" stroke=\"#000000\" points=\"241.5,-187 124.5,-187 124.5,-104 241.5,-104 241.5,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Time &lt;= 9.0</text>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.794</text>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 138</text>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [105, 33]</text>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M310.7869,-225.5349C291.4604,-213.4305 270.0705,-200.0336 250.4074,-187.7183\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"252.1366,-184.6716 241.8038,-182.3298 248.421,-190.6041 252.1366,-184.6716\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#fffdfd\" stroke=\"#000000\" points=\"428,-187 318,-187 318,-104 428,-104 428,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Time &lt;= 41.5</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.0</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 163</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [82, 81]</text>\n",
       "<text text-anchor=\"middle\" x=\"373\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M373,-222.8796C373,-214.6838 373,-205.9891 373,-197.5013\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"376.5001,-197.298 373,-187.2981 369.5001,-197.2981 376.5001,-197.298\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#efb185\" stroke=\"#000000\" points=\"110,-68 0,-68 0,0 110,0 110,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.851</text>\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 94</text>\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [68, 26]</text>\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M135.3375,-103.9815C124.4703,-94.5151 112.9295,-84.462 102.0865,-75.0168\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.1734,-72.1929 94.3341,-68.2637 99.5755,-77.4712 104.1734,-72.1929\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#ea995e\" stroke=\"#000000\" points=\"238,-68 128,-68 128,0 238,0 238,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.632</text>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 44</text>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [37, 7]</text>\n",
       "<text text-anchor=\"middle\" x=\"183\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M183,-103.9815C183,-95.618 183,-86.7965 183,-78.3409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.5001,-78.2636 183,-68.2637 179.5001,-78.2637 186.5001,-78.2636\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#abd6f4\" stroke=\"#000000\" points=\"366,-68 256,-68 256,0 366,0 366,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.948</text>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 101</text>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [37, 64]</text>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = Yes</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M349.9135,-103.9815C345.0585,-95.2504 339.926,-86.0202 335.0371,-77.2281\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"337.9712,-75.3025 330.0524,-68.2637 331.8533,-78.7043 337.9712,-75.3025\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#efb184\" stroke=\"#000000\" points=\"494,-68 384,-68 384,0 494,0 494,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.847</text>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 62</text>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [45, 17]</text>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M397.576,-103.9815C402.7986,-95.1585 408.3229,-85.8258 413.5763,-76.9506\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"416.6364,-78.652 418.7184,-68.2637 410.6126,-75.0863 416.6364,-78.652\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#e99355\" stroke=\"#000000\" points=\"685.5,-187 568.5,-187 568.5,-104 685.5,-104 685.5,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Recency &lt;= 20.5</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.537</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 253</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [222, 31]</text>\n",
       "<text text-anchor=\"middle\" x=\"627\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M627,-222.8796C627,-214.6838 627,-205.9891 627,-197.5013\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"630.5001,-197.298 627,-187.2981 623.5001,-197.2981 630.5001,-197.298\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#e6843e\" stroke=\"#000000\" points=\"871,-187 761,-187 761,-104 871,-104 871,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Time &lt;= 17.5</text>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.156</text>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 44</text>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [43, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M685.5064,-227.6626C706.7537,-214.2847 730.855,-199.1098 752.5122,-185.4738\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"754.3976,-188.4227 760.9951,-180.1327 750.6679,-182.4991 754.3976,-188.4227\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#e99254\" stroke=\"#000000\" points=\"629.5,-68 512.5,-68 512.5,0 629.5,0 629.5,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"571\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.527</text>\n",
       "<text text-anchor=\"middle\" x=\"571\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 252</text>\n",
       "<text text-anchor=\"middle\" x=\"571\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [222, 30]</text>\n",
       "<text text-anchor=\"middle\" x=\"571\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M606.1477,-103.9815C601.7625,-95.2504 597.1267,-86.0202 592.711,-77.2281\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"595.8246,-75.6291 588.2087,-68.2637 589.5692,-78.7708 595.8246,-75.6291\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#399de5\" stroke=\"#000000\" points=\"742.5,-68 647.5,-68 647.5,0 742.5,0 742.5,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"695\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"695\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"695\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"695\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = Yes</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M652.3207,-103.9815C657.7016,-95.1585 663.3932,-85.8258 668.8059,-76.9506\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"671.8851,-78.6236 674.1038,-68.2637 665.9088,-74.9789 671.8851,-78.6236\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#e78b48\" stroke=\"#000000\" points=\"871,-68 761,-68 761,0 871,0 871,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.371</text>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 14</text>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [13, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M816,-103.9815C816,-95.618 816,-86.7965 816,-78.3409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"819.5001,-78.2636 816,-68.2637 812.5001,-78.2637 819.5001,-78.2636\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"#000000\" points=\"991,-68 889,-68 889,0 991,0 991,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"940\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"940\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 30</text>\n",
       "<text text-anchor=\"middle\" x=\"940\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [30, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"940\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = No</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M862.173,-103.9815C872.7007,-94.5151 883.8808,-84.462 894.3849,-75.0168\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"896.7994,-77.5526 901.8951,-68.2637 892.1189,-72.3475 896.7994,-77.5526\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fc85722eed0>"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include decision tree visualization here\n",
    "dot_data = tree.export_graphviz(clf.best_estimator_,\n",
    "                                feature_names=[\"Recency\", \"Frequency\", \"Monetary\", \"Time\"],  \n",
    "                                class_names=[\"No\", \"Yes\"],\n",
    "                                out_file=None, \n",
    "                                filled=True, \n",
    "                                max_depth=3)\n",
    "graphviz.Source(dot_data, format=\"png\")\n",
    "\n",
    "# Discuss what the model has learned\n",
    "# * See below graph/tree output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buUqTdTWGuDU"
   },
   "source": [
    "So this model was trying to predict whether or not a person donated blood in a particular month based on things like past frequency, recency, time since first donation, and how much they have donated in total.\n",
    "\n",
    "The model learned that if a person has donated in the last 6.5 months, has donated more than 3.5 times, and donated for the first time less than 3.5 years ago, then they are far more likely to donate that month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQEEzBUUmlkX"
   },
   "source": [
    "## 6. (optional 5% extra credit) Implement reduced error pruning to help avoid overfitting.  \n",
    "- You will need to take a validation set out of your training data to do this, while still having a test set to test your final accuracy. \n",
    "- Create a table comparing your decision tree implementation's results on the cars and voting data sets with and without reduced error pruning. \n",
    "- This table should compare:\n",
    "    - a) The # of nodes (including leaf nodes) and tree depth of the final decision trees \n",
    "    - b) The generalization (test set) accuracy. (For the unpruned 10-fold CV models, just use their average values in the table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdEJ4cZImlkX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab_3_decision_tree.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
